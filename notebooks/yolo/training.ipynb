{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Vision mit Yolov5\n",
    "\n",
    "Yolov5 ist eine Objekterkennungs Architektur in der fünften Version die von der Firma Ultralytics kostenlos bereitgestellt wird. \n",
    "\n",
    "Die Software kann unter der GNU General Public License v3.0 unter gewissen vorraussetzungen auch Kommerziel genutzt werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erste Schritte\n",
    "#### Setup in conda\n",
    "1. Eine Conda umgebung in der Python Version 3.7.11 wird benötigt\n",
    "2. in der conda Umgebung pip, jupyter, cudatoolkit und lxml installieren\n",
    "3. -> conda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cudatoolkit=10.1 -c pytorch\n",
    "4. in den aktuellen yolov5 ordner wechseln mit \"cd .\\yolov5\\\"\n",
    "5. benötigte bibliotheken mit \"pip install -r requirements.txt\" installieren\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Repository kann über diesen [Link](https://github.com/ultralytics/yolov5) heruntergeladen werden. Alternativ kann es auch über Git geklont werden.\n",
    "Hier sind schon alle Dateien die benötigt werden also ist der vorhgerige Schritt hier nicht nötig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "import cv2 \n",
    "import os\n",
    "import glob\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Startet die Kamera zum Bilder erstellen\n",
    "\n",
    "  \n",
    "# Videoaufnahme\n",
    "def useCameraForPictureCapture(train = True):\n",
    "    # Festlegen der Bildernamen\n",
    "    iTrain = len(os.listdir('./yolov5/data/images/train/images'))\n",
    "    iValidation = len(os.listdir('./yolov5/data/images/validation/images'))\n",
    "    # Auslesen der Kamera\n",
    "    vid = cv2.VideoCapture(0) \n",
    "    \n",
    "    while(True): \n",
    "        ret, frame = vid.read() \n",
    "\n",
    "        cv2.imshow('frame', frame) \n",
    "        if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "            if train:\n",
    "                cv2.imwrite('./yolov5/data/images/train/images/' + f'{iTrain:05d}'+ '.jpeg', frame)\n",
    "                iTrain+=1   \n",
    "            else:\n",
    "                cv2.imwrite('./yolov5/data/images/validation/images/' + f'{iValidation:05d}'+ '.jpeg', frame)\n",
    "                iValidation+=1\n",
    "            \n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "            break\n",
    "        \n",
    "    \n",
    "    vid.release() \n",
    "    cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsbilder aufnehmen (training set)\n",
    "\n",
    "Hierbei ist es überproportional wichtig die Klassen (Objekte) die erkannt werden sollen, möglichst von allen Seiten und mit unterschiedlichem Hintergründen zu fotografieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useCameraForPictureCapture()\n",
    "# Fenster schließen = q Taste\n",
    "# Bild aufnehmen = s Taste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validierungsbilder aufnehmen (validation set) \n",
    "Ein gutes Verhältnis zwischen Trainings- und Validierungsbildern ist Beispielsweise 5:1 (5 Trainingsbilder pro Validierungsbild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useCameraForPictureCapture(False)\n",
    "# Fenster schließen = q Taste\n",
    "# Bild aufnehmen = s Taste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Festlegen der Klassen\n",
    "\n",
    "Um den Bildern Klassen zuordnen zu können, müssen zunächst die sogenannten Klassen festgelegt werden.\n",
    "Dazu müssen zwei Dateien bearbeitet werden.\n",
    "\n",
    "1. Einfügen der Anzahl der Klassen und Klassennamen in der \"/yolov5/data/coco128.yaml\" Datei\n",
    "2. In der Datei \"/OpenLabeling/main/class_list.txt\" müssen die Klassennamen auch aufgeführt werden. Hier pro Klassenname eine Zeile (Keine Kommata)\n",
    "\n",
    "\n",
    "### Klassifizierung (labeling) der Bilder\n",
    "Wichtig beim klassifizieren der Bilder ist es die Kästen (Bounding Boxes) möglichst genau um das Objekt zu zeichnen, da Ungenauigkeiten beim klassifizieren auch zu Ungenauigkeiten bei der Objekterkennung führen.\n",
    "\n",
    "Steuerung des Tools:\n",
    "\n",
    "<img src=\"OpenLabeling\\keyboard_usage.jpg\" alt=\"keyboardUsage\" title=\"Keyboard Usage\" width=\"150\" height=\"100\" />\n",
    "\n",
    "Die beiden commandos unten legen die Eingabepfade für die Bilder und die Ausgabepfade für die Labels fest. (Können prinzipiell genauso in der Konsole ausgeführt werden solange das Machine-Vision verzeichniss geöffnet ist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python OpenLabeling/main/main.py --input_dir ../../yolov5/data/images/train/images --output_dir ../../yolov5/data/images/train/labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klassifizierung der Validierungsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python OpenLabeling/main/main.py --input_dir ../../yolov5/data/images/validation/images --output_dir ../../yolov5/data/images/validation/labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainieren des Models\n",
    "\n",
    "Als letzter Schritt muss dass Model in Yolo Trainiert werden.\n",
    "\n",
    "Dabei können unterschiedliche Parametern (sogenannten Flags) spezifiziert werden. \n",
    "\n",
    "Hier sind einige Beispiele für Flags in Yolov5:\n",
    "1. \"--data ...\" spezifiziert die yaml Datei die zum trainieren genutzt wird\n",
    "2. \"--batch-size ...\" spezifiziert die Anzahl der Datenpunkte die durch das Netzwerk geführt werden\n",
    "3. \"--epochs ...\" spezifiziert die Anzahl der Trainierten Epochen\n",
    "4. \"--weights ...\" spezifiziert das vortrainierte yolo dataset welches genutzt wird um das\n",
    "\n",
    "Bevor das Model trainiert werden kann, müssen die Label noch in den Korrekten Ordner verschoben werden. Dies wird von den beiden unteren schleifen erledigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in glob.glob(os.path.join(\"yolov5/data/images/train/labels/YOLO_darknet/\", '*.*')):\n",
    "    shutil.copy(filename, \"yolov5/data/images/train/labels/\")\n",
    "\n",
    "for filename in glob.glob(os.path.join(\"yolov5/data/images/validation/labels/YOLO_darknet/\", '*.*')):\n",
    "    shutil.copy(filename, \"yolov5/data/images/validation/labels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python yolov5/train.py --batch 20 --epochs 10 --data coco128.yaml --weights yolov5s.pt"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad3b900c16faf988d2bab2d4031b590f2e1d55f14c71ca822df608f53102f029"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
